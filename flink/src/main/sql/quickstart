set sql-client.execution.result-mode = tableau;

CREATE TABLE hudi_table(
    ts BIGINT,
    uuid VARCHAR(40) PRIMARY KEY NOT ENFORCED,
    rider VARCHAR(20),
    driver VARCHAR(20),
    fare DOUBLE,
    city VARCHAR(20)
)
PARTITIONED BY (`city`)
WITH (
  'connector' = 'hudi',
  'path' = 'file:///tmp/hudi_table',
  'table.type' = 'MERGE_ON_READ',
  'write.operation' = 'upsert',
  'write.ignore.failed' = 'true'
);
-- insert data using values
INSERT INTO hudi_table
VALUES
(1695159649087,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),
(1695091554788,'e96c4396-3fad-413a-a942-4cb36106d721','rider-C','driver-M',27.70 ,'san_francisco'),
(1695046462179,'9909a8b1-2d15-4d3d-8ec9-efc48c536a00','rider-D','driver-L',33.90 ,'san_francisco'),
(1695332066204,'1dced545-862b-4ceb-8b43-d2a568f6616b','rider-E','driver-O',93.50,'san_francisco'),
(1695516137016,'e3cf430c-889d-4015-bc98-59bdce1e530c','rider-F','driver-P',34.15,'sao_paulo'),
(1695376420876,'7a84095f-737f-40bc-b62f-6b69664712d2','rider-G','driver-Q',43.40 ,'sao_paulo'),
(1695173887231,'3eeb61f7-c2b0-4636-99bd-5d7a5a1d2c04','rider-I','driver-S',41.06 ,'chennai'),
(1695115999911,'c8abbe79-8d89-47ea-b4ce-4d224bae5bfa','rider-J','driver-T','17.85','chennai');

SET 'execution.runtime-mode' = 'batch';
UPDATE hudi_table SET fare = 25.0 WHERE uuid = '334e26e9-8355-45cc-97c6-c31daf0df330';
-- Query the table in stream mode in another shell to see change logs
SET 'execution.runtime-mode' = 'streaming';
select * from hudi_table/*+ OPTIONS('read.streaming.enabled'='true')*/;


set sql-client.execution.result-mode = tableau;

CREATE TABLE hudi_table(
    ts BIGINT,
    uuid VARCHAR(40) PRIMARY KEY NOT ENFORCED,
    rider VARCHAR(20),
    driver VARCHAR(20),
    fare DOUBLE,
    city VARCHAR(20)
)
PARTITIONED BY (`city`)
WITH (
  'connector' = 'hudi',
  'path' = 'file:///tmp/hudi_table',
  'table.type' = 'MERGE_ON_READ',
  'write.operation' = 'upsert',
  'changelog.enabled' = 'true',  -- this option enable the change log enabled
  'cdc.enabled' = 'true' -- this option enable the cdc log enabled
);
-- insert data using values
INSERT INTO hudi_table
VALUES
(1695159649087,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),
(1695091554788,'e96c4396-3fad-413a-a942-4cb36106d721','rider-C','driver-M',27.70 ,'san_francisco'),
(1695046462179,'9909a8b1-2d15-4d3d-8ec9-efc48c536a00','rider-D','driver-L',33.90 ,'san_francisco'),
(1695332066204,'1dced545-862b-4ceb-8b43-d2a568f6616b','rider-E','driver-O',93.50,'san_francisco'),
(1695516137016,'e3cf430c-889d-4015-bc98-59bdce1e530c','rider-F','driver-P',34.15,'sao_paulo'),
(1695376420876,'7a84095f-737f-40bc-b62f-6b69664712d2','rider-G','driver-Q',43.40 ,'sao_paulo'),
(1695173887231,'3eeb61f7-c2b0-4636-99bd-5d7a5a1d2c04','rider-I','driver-S',41.06 ,'chennai'),
(1695115999911,'c8abbe79-8d89-47ea-b4ce-4d224bae5bfa','rider-J','driver-T',17.85,'chennai');

SET 'execution.runtime-mode' = 'batch';
UPDATE hudi_table SET fare = 25.0 WHERE uuid = '334e26e9-8355-45cc-97c6-c31daf0df330';
-- Query the table in stream mode in another shell to see change logs
SET 'execution.runtime-mode' = 'streaming';
select * from hudi_table/*+ OPTIONS('read.streaming.enabled'='true')*/;



DataTypes.FIELD("uuid", DataTypes.VARCHAR(20)),// record key
							DataTypes.FIELD("name", DataTypes.VARCHAR(10)),
							DataTypes.FIELD("age", DataTypes.INT()),
							DataTypes.FIELD("ts", DataTypes.TIMESTAMP(3)), // precombine field
							DataTypes.FIELD("partition", DataTypes.VARCHAR(10)))


set sql-client.execution.result-mode = tableau;

drop table hudi_table1;
CREATE TABLE hudi_table3(
    uuid VARCHAR(40) PRIMARY KEY NOT ENFORCED,
    name VARCHAR(20),
    age INT,
    ts TIMESTAMP,
    `partition` VARCHAR(10)
)
PARTITIONED BY (`partition`)
WITH (
  'connector' = 'hudi',
  'path' = 'file:///tmp/hudi_table_test2',
  'table.type' = 'MERGE_ON_READ'
);



SET 'execution.runtime-mode' = 'streaming';
SET 'sql-client.execution.result-mode' = 'table';
SET 'sql-client.execution.max-table-result.rows' = '10000';
SET 'parallelism.default' = '1';
SET 'pipeline.auto-watermark-interval' = '200';
SET 'pipeline.max-parallelism' = '10';
SET 'table.exec.state.ttl' = '1000';
SET 'restart-strategy.type' = 'fixed-delay';
SET 'table.optimizer.join-reorder-enabled' = 'true';
SET 'table.exec.spill-compression.enabled' = 'true';
SET 'table.exec.spill-compression.block-size' = '128kb';

SET 'state.checkpoints.dir' = 'file:///tmp/sql_checkpoint';
SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';
SET 'execution.checkpointing.interval' = '10 s';
SET 'execution.checkpointing.min-pause' = '1 s';
SET 'execution.checkpointing.max-concurrent-checkpoints' = '1';
SET 'execution.checkpointing.prefer-checkpoint-for-recovery' = 'true';


CREATE TABLE t_user (
    id INT,
    name STRING,
    age INT,
    sex BOOLEAN,
    birth DATE,
    PRIMARY KEY (id) NOT ENFORCED
) PARTITIONED BY (birth)
WITH (

    'connector' = 'hudi',
    'path' = 'file:///tmp/t_user',
    'table.type' = 'MERGE_ON_READ',
    'hoodie.datasource.write.hive_style_partitioning'='true',
    'hoodie.datasource.write.partitionpath.field'='birth',
    'hoodie.datasource.write.recordkey.field'='id',
    'write.partition.format'='yyyy-MM-dd',
    'compaction.tasks'='3',
    'compaction.delta_seconds'='3600',
    'compaction.delta_commits'='5',
    'compaction.trigger.strategy'='num_or_time',
    'hoodie.metadata.log.compaction.enable'='true',
    'hoodie.clean.async'='true',
    'hoodie.clean.automatic'='true',
    'hoodie.clean.trigger.strategy'='NUM_COMMITS',
    'hoodie.clean.max.commits'='1000',
    'clean.policy'='KEEP_LATEST_COMMITS',
    'hoodie.cleaner.commits.retained'='100',
    'read.streaming.enabled'='true',
    'read.streaming.check-interval'='60',
    'read.start-commit'='20231025001021',
    'hoodie.index.type' = 'BUCKET',
    'hoodie.bucket.index.hash.field' = 'id',
    'hoodie.bucket.index.min.num.buckets' = '1',
    'hoodie.bucket.index.max.num.buckets' = '8',
    'hoodie.bucket.index.split.threshold' = 2.0,
    'write.operation' = 'upsert',
    'clustering.schedule.enabled' = 'true'
);

        options.put(FlinkOptions.CLUSTERING_PLAN_STRATEGY_CLASS.key(), FlinkConsistentBucketClusteringPlanStrategy.class.getName());
        options.put(HoodieClusteringConfig.EXECUTION_STRATEGY_CLASS_NAME.key(), "org.apache.hudi.client.clustering.run.strategy.SparkConsistentBucketClusteringExecutionStrategy");
        return options;

CREATE TABLE t_user_datagen (
      id INT,
      name STRING,
      age INT,
      sex BOOLEAN,
      birth DATE
)
WITH (
    'connector' = 'datagen',
    'rows-per-second'='1'
);

insert into t_user select * from t_user_datagen;